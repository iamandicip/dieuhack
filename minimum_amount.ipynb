{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "datafolder = \"../train/\"\n",
    "assert path.isdir(datafolder)\n",
    "\n",
    "data1folder = '../final_test'\n",
    "assert path.isdir(data1folder)\n",
    "\n",
    "html1_folder = path.join(data1folder, \"html\")\n",
    "assert path.isdir(html1_folder)\n",
    "\n",
    "html_folder = path.join(datafolder, \"html\")\n",
    "assert path.isdir(html_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>ISSUER.NAME</th>\n",
       "      <th>ZCP.FL</th>\n",
       "      <th>MIN.TRAD.AMT</th>\n",
       "      <th>MLT.TRAD.AMT</th>\n",
       "      <th>OPS.CURR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARCBAS031621</td>\n",
       "      <td>CIUDAD DE BUENOS AIRES</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>ARS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT0000248448</td>\n",
       "      <td>UNICREDIT BANK AUSTRIA AG</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>EUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT0000A0MPB1</td>\n",
       "      <td>BAWAG PSK BANK FUR ARBEIT UND WIRTSCHAFT UND O...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>EUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT0000A17HH9</td>\n",
       "      <td>RAIFFEISEN CENTROBANK AG</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT0000A1HE76</td>\n",
       "      <td>RAIFFEISEN CENTROBANK AG</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>CZK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ISIN                                        ISSUER.NAME ZCP.FL  \\\n",
       "0  ARCBAS031621                             CIUDAD DE BUENOS AIRES      N   \n",
       "1  AT0000248448                          UNICREDIT BANK AUSTRIA AG      N   \n",
       "2  AT0000A0MPB1  BAWAG PSK BANK FUR ARBEIT UND WIRTSCHAFT UND O...      N   \n",
       "3  AT0000A17HH9                           RAIFFEISEN CENTROBANK AG      N   \n",
       "4  AT0000A1HE76                           RAIFFEISEN CENTROBANK AG      Y   \n",
       "\n",
       "   MIN.TRAD.AMT  MLT.TRAD.AMT OPS.CURR  \n",
       "0             0          1000      ARS  \n",
       "1             0        100000      EUR  \n",
       "2             0           100      EUR  \n",
       "3             0          1000      USD  \n",
       "4             0          1000      CZK  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_csv( datafolder + \"outcome/ISIN_train.csv\", header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "files_isins = pd.read_csv( datafolder + \"docID/docid_train.csv\", header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "\n",
    "def get_isin_for_file(labels, file_name, print_result = False):\n",
    "    isin = None\n",
    "    #function to get the isin from the labels, given a file name\n",
    "    file_id = file_name.split('_')[0]\n",
    "    idx = labels['DOCID'] == file_id\n",
    "    isins = labels['ISIN'][idx]\n",
    "    if isins.values: \n",
    "        isin = isins.values[0]\n",
    "    \n",
    "    if(print_result):\n",
    "        print(\"%s - %s\" % (file_name, isin))\n",
    "    return isin\n",
    "\n",
    "def get_labels_for_isin(labels, isin):\n",
    "    idx = labels['ISIN'] == isin\n",
    "    return labels[:][idx].values\n",
    "\n",
    "def get_label_value_for_isin(labels, isin, attr_name):\n",
    "    #function to return the value of the label for a given isin\n",
    "    idx = labels['ISIN'] == isin\n",
    "    return labels[attr_name][idx].values[0]\n",
    "\n",
    "def save_file(isin, text):\n",
    "    f = open(isin + '.txt', 'w')\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "\n",
    "def document_to_words(doc_path):\n",
    "    with open(doc_path, 'r', encoding='utf8') as html_file:\n",
    "        full_text = html_file.read()\n",
    "        body = re.findall(r'<body[^>]*?>(.*?)</body>', full_text)\n",
    "        cleantext = re.sub('<\\/?span[^>]*>', ' ', body[0])\n",
    "        clean_text = re.sub('<.*?>', ' ', cleantext)\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = re.sub('0.01', '1', clean_text)\n",
    "        clean_text = re.sub(',', '', clean_text)\n",
    "        clean_text = re.sub(r'[^a-z0-9]', ' ', clean_text)\n",
    "        clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "        words = nltk.word_tokenize(clean_text)\n",
    "        stops = set(stopwords.words('english'))\n",
    "        relevant_words = [w for w in words if w not in stops and w != '' and w != ' ']\n",
    "        return relevant_words\n",
    "\n",
    "keywords_contexts = [['minimum', 'principal', 'amounts'], \\\n",
    "                     ['minimum', 'amounts'], \\\n",
    "                     ['minimum', 'amount'], \\\n",
    "#                      ['minimum']\n",
    "                    ]\n",
    "\n",
    "def contains(small_list, big_list):\n",
    "    for word in small_list:\n",
    "        if word not in big_list:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def keywords_in_context(keywords, context):\n",
    "    for keywords_list in keywords:\n",
    "        if contains(keywords_list, context):\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def sort_contexts(contexts):\n",
    "    c = sorted(contexts, key=len)\n",
    "    c.reverse()\n",
    "    return c\n",
    "\n",
    "keywords_contexts = sort_contexts(keywords_contexts)\n",
    "\n",
    "def extract_relevant_info(words, op_curr, window_before, window_after):\n",
    "#     print('extracting info for : %s - %s', (op_curr, words))\n",
    "\n",
    "    AMOUNT_REGEX = r'\\d{1,12}'\n",
    "    CURR_AMOUNT_REGEX = r'[a-z]{3}\\d{1,12}'\n",
    "    \n",
    "    relevant_info = ['0']\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "\n",
    "        if word == op_curr and i + window_after < len(words):\n",
    "            #retain look_around words around the keyword\n",
    "            context_before = words[i - window_before : i]\n",
    "            \n",
    "            keywords_present = keywords_in_context(keywords_contexts, context_before)\n",
    "            \n",
    "            if keywords_present:\n",
    "                context_after = words[i : i + window_after]\n",
    "\n",
    "                for w in context_after:\n",
    "                    if re.match(AMOUNT_REGEX, w):\n",
    "\n",
    "                            context = context_before + context_after\n",
    "\n",
    "                            relevant_info += context\n",
    "\n",
    "                            break\n",
    "\n",
    "        elif word.startswith(op_curr) and re.match(CURR_AMOUNT_REGEX, word):\n",
    "            context_before = words[i - window_before : i]\n",
    "            \n",
    "            keywords_present = keywords_in_context(keywords_contexts, context_before)\n",
    "            \n",
    "            if keywords_present:\n",
    "                context_after = words[i : i + window_after]\n",
    "\n",
    "                matches = re.findall(AMOUNT_REGEX, word)\n",
    "\n",
    "                context = context_before + [op_curr] + matches + context_after\n",
    "\n",
    "                relevant_info += context\n",
    "                \n",
    "    return ' '.join(relevant_info)\n",
    "\n",
    "inspect = []\n",
    "\n",
    "def group_docs_by_isin(html_folder, labels, files_isins, pickle_results=False, pickle_file='docs_by_isin.pickle'):\n",
    "    t0 = time()\n",
    "    \n",
    "    contents_by_isin = {}\n",
    "    \n",
    "    print('Processing files')\n",
    "\n",
    "    for file_name in os.listdir(html_folder):\n",
    "        \n",
    "        print('.', end='')\n",
    "        \n",
    "        if file_name.endswith(\".html\"):\n",
    "\n",
    "            #first find the isin corresponding to this file\n",
    "            isin = get_isin_for_file(files_isins, file_name)\n",
    "\n",
    "            if(isin is not None):\n",
    "\n",
    "                #found the isin to associate the document with\n",
    "                words = document_to_words(path.join(html_folder, file_name))\n",
    "                \n",
    "                op_curr = get_label_value_for_isin(labels, isin, 'OPS.CURR')\n",
    "                \n",
    "                window_before = 4\n",
    "                window_after = 2\n",
    "                \n",
    "                if isin in inspect:\n",
    "                    save_file(isin, ' '.join(words))\n",
    "                \n",
    "#                 file_content = extract_relevant_info(words, op_curr.lower(), window_before, window_after)\n",
    "                file_content = ' '.join(words)\n",
    "\n",
    "                try:\n",
    "                    #if there is already existing data for this isin, append the new data\n",
    "                    existing_isin_data = contents_by_isin[isin]\n",
    "                    contents_by_isin[isin] = existing_isin_data + file_content\n",
    "\n",
    "                except KeyError:\n",
    "                    contents_by_isin[isin] = file_content\n",
    "    \n",
    "    print('\\nFinished grouping file contents indexed by ISIN')\n",
    "    \n",
    "    if pickle_results:\n",
    "        pickle.dump(contents_by_isin, open(pickle_file, 'wb'))\n",
    "        \n",
    "        print('Saved file contents indexed by ISIN to:', pickle_file)\n",
    "        \n",
    "    print('Processed %d files in %0.3fs' % (len(contents_by_isin.keys()), (time() - t0)))\n",
    "        \n",
    "    return contents_by_isin\n",
    "\n",
    "def load_and_sort_training_data(file_name):\n",
    "    data = pickle.load(open(file_name, 'rb'))\n",
    "    data_by_isin = [[k, v] for k, v in data.items()]\n",
    "    data_by_isin_df = pd.DataFrame(data_by_isin, columns = ['isin','content'])\n",
    "    data_by_isin_df.sort_values('isin', axis=0, inplace=True)\n",
    "  \n",
    "    dataset = data_by_isin_df.as_matrix(columns=['content'])\n",
    "    \n",
    "    labels = pd.read_csv( datafolder + 'outcome/ISIN_train.csv', header=0, sep=',', quoting=1, thousands=\",\")\n",
    "    data_labeled_df = pd.DataFrame(labels[['ISIN','MIN.TRAD.AMT']])\n",
    "    data_labeled_df.set_index(['ISIN'])\n",
    "    data_labeled_df = data_labeled_df.drop_duplicates()\n",
    "    data_labeled_df.sort_values('ISIN', axis=0, inplace=True)\n",
    "    labelset = data_labeled_df.as_matrix(columns=['MIN.TRAD.AMT'])\n",
    "    \n",
    "    return dataset.ravel(), labelset.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files\n",
      "..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Finished grouping file contents indexed by ISIN\n",
      "Saved file contents indexed by ISIN to: min_labeled_docs_by_isin.pickle\n",
      "Processed 5456 files in 288.450s\n",
      "OK!\n"
     ]
    }
   ],
   "source": [
    "group_docs_by_isin(html_folder, labels, files_isins, pickle_results=True, pickle_file='min_labeled_docs_by_isin.pickle')\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'instrument summary cd buenos aires 3 98 titulos de deuda publica 2014 15 3 18 class 3 issuer gk625220 cuidad autonoma de buenos aires domicile argentina sector cities municipal authorities basic data swiss national security number 26209866 isin arcbas031621 cfi code dbftxr assigned six within six jurisdiction liquid market 82 buenos aires instrument type debt short name 3 98 buenos 18 3 original issuer gk625220 cd buenos aires argentina currency principal usd 100000000 outstanding capital maturity date 15 03 2018 callability unknown coupon 3 98 fixed payment frequency every 6 months income type periodical payment first payment per year 15 03 inflation protected denominations usd 1000 depository custody cvba clearstream bk lux euroclear bank latest instrument ratings scheme name date rating rating trend mdyltdfc moody 39 long term debt foreign ccy ratings 30 11 2015 caa1 mdyltdlc moody 39 long term debt local ccy ratings 04 11 2015 caa1 mdyendc rating history moody 39 endorsement compliance 26 11 2014 1b latest corporate actions 15 09 2015 interest interest rate 3 98 payment date 15 09 2015 entitlement period 15 03 2015 14 09 2015 taxes france n p 15 03 2015 interest interest rate 3 98 payment date 15 03 2015 entitlement period 15 09 2014 14 03 2015 taxes france n p 15 09 2014 interest interest rate 3 98 payment date 15 09 2014 entitlement period 15 03 2014 14 09 2014 taxes france n p 15 03 2014 interest interest rate 3 98 payment date 15 03 2014 entitlement period 15 09 2013 14 03 2014 taxes france n p 15 09 2013 interest interest rate 3 98 payment date 15 09 2013 entitlement period 15 03 2013 14 09 2013 taxes france n p latest corporate actions company 10 05 2008 company amalgamation takeover purchase offer 28 11 2003 company amalgamation takeover purchase offer 30 09 2003 company amalgamation takeover purchase offer 23 07 2003 company amalgamation takeover purchase offer 02 07 2003 company amalgamation takeover purchase offer data version t20 run 21 jan 2016 vdfpuls y2016021 t10t l v0042mpzhwvdb10']\n"
     ]
    }
   ],
   "source": [
    "dataset, labelset = load_and_sort_training_data('min_labeled_docs_by_isin.pickle')\n",
    "print(dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "dataset, labelset = load_and_sort_training_data('min_labeled_docs_by_isin.pickle')\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(dataset, labelset,\\\n",
    "                                                                     test_size=0.3, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \\\n",
    "                     ('tfidf', TfidfTransformer()), \\\n",
    "                     ('clf', SGDClassifier(average=True))])\n",
    "\n",
    "text_clf = text_clf.set_params(clf__alpha = 1e-04, \\\n",
    "                               clf__n_iter = 100, \\\n",
    "                               clf__penalty = 'l2', \\\n",
    "                               clf__n_jobs = -1, \\\n",
    "                               tfidf__norm = 'l2', \\\n",
    "                               tfidf__use_idf = True, \\\n",
    "                               vect__max_df = 0.4, \\\n",
    "                               vect__ngram_range = (1, 2))\n",
    "\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = text_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(np.mean(y_train == y_train_pred))\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922419059255\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.98      0.96      1441\n",
      "         10       0.00      0.00      0.00         1\n",
      "        200       1.00      1.00      1.00         2\n",
      "        250       0.00      0.00      0.00         0\n",
      "       1000       1.00      0.38      0.55         8\n",
      "       2000       0.72      0.79      0.75        52\n",
      "       5000       0.00      0.00      0.00         4\n",
      "      10000       0.50      0.12      0.20         8\n",
      "      20000       0.75      0.60      0.67         5\n",
      "      50000       0.00      0.00      0.00         3\n",
      "     100000       0.83      0.44      0.57        55\n",
      "     104300       0.00      0.00      0.00         1\n",
      "     120000       0.00      0.00      0.00         1\n",
      "     130000       0.00      0.00      0.00         1\n",
      "     150000       0.60      0.60      0.60        10\n",
      "     200000       0.43      0.36      0.39        25\n",
      "     250000       1.00      0.17      0.29         6\n",
      "     300000       0.00      0.00      0.00         1\n",
      "     500000       0.00      0.00      0.00         3\n",
      "    1000000       0.00      0.00      0.00         2\n",
      "   50000000       0.00      0.00      0.00         1\n",
      "  830000000       0.00      0.00      0.00         0\n",
      " 1000000000       1.00      0.25      0.40         4\n",
      " 1250000000       0.00      0.00      0.00         1\n",
      " 2020000000       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.91      0.92      0.91      1637\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ciprian/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ciprian/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(text_clf.score(X_test, y_test))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileId</th>\n",
       "      <th>isin</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0900045c839f0d17</td>\n",
       "      <td>AT0000A0MPB1</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0900045c83f01550</td>\n",
       "      <td>AU3FN0030839</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0900045c83f8dc8e</td>\n",
       "      <td>AU3FN0030912</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0900045c838bd72f</td>\n",
       "      <td>CH0254068866</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0900045c83bae03d</td>\n",
       "      <td>CH0254071365</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0900045c83bba3a8</td>\n",
       "      <td>CH0254071431</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0900045c8398c206</td>\n",
       "      <td>CH0266691317</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0900045c83f93596</td>\n",
       "      <td>CH0266712170</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0900045c83855f37</td>\n",
       "      <td>CH0266718706</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0900045c83f085ae</td>\n",
       "      <td>CH0266720322</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0900045c831515d2</td>\n",
       "      <td>CH0266741278</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0900045c8303f377</td>\n",
       "      <td>CH0266741732</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0900045c840e26ab</td>\n",
       "      <td>CH0266742722</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0900045c8311fa5c</td>\n",
       "      <td>CH0266743654</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0900045c83516e3f</td>\n",
       "      <td>CH0266744942</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0900045c83070e18</td>\n",
       "      <td>CH0273393162</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0900045c831a1d0c</td>\n",
       "      <td>CH0273396603</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0900045c831dd9d9</td>\n",
       "      <td>CH0274763686</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0900045c830bf519</td>\n",
       "      <td>CH0277773989</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0900045c83cde80b</td>\n",
       "      <td>CH0279920281</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0900045c833eec1d</td>\n",
       "      <td>CH0279923616</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0900045c834e20c1</td>\n",
       "      <td>CH0279924770</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0900045c8314646b</td>\n",
       "      <td>CH0281105046</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0900045c835cfdd0</td>\n",
       "      <td>CH0283709548</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0900045c8359dcd9</td>\n",
       "      <td>CH0283709647</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0900045c833b4163</td>\n",
       "      <td>CH0283710165</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0900045c83790c62</td>\n",
       "      <td>CH0283715131</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0900045c8398bb45</td>\n",
       "      <td>CH0283716741</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0900045c8342507e</td>\n",
       "      <td>CH0284402853</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0900045c83470599</td>\n",
       "      <td>CH0285315823</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0900045c82b8065f</td>\n",
       "      <td>XS1140692481</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0900045c82c0b0ba</td>\n",
       "      <td>XS1141959368</td>\n",
       "      <td>250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0900045c83e252ef</td>\n",
       "      <td>XS1143231527</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0900045c8318b8d6</td>\n",
       "      <td>XS1143236831</td>\n",
       "      <td>250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0900045c82d17828</td>\n",
       "      <td>XS1167051256</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0900045c82dd35f3</td>\n",
       "      <td>XS1173869022</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0900045c82cd0a3b</td>\n",
       "      <td>XS1173870111</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0900045c82f0734f</td>\n",
       "      <td>XS1182591575</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0900045c831ac2a2</td>\n",
       "      <td>XS1184956834</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0900045c8326be41</td>\n",
       "      <td>XS1184959770</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0900045c82d837d5</td>\n",
       "      <td>XS1189246140</td>\n",
       "      <td>2020000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0900045c8394a39d</td>\n",
       "      <td>XS1190837960</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0900045c839c90d3</td>\n",
       "      <td>XS1190839073</td>\n",
       "      <td>500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0900045c838a58b8</td>\n",
       "      <td>XS1190839156</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0900045c8389117d</td>\n",
       "      <td>XS1190839743</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0900045c83889f1c</td>\n",
       "      <td>XS1190840162</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0900045c8364ac0d</td>\n",
       "      <td>XS1190843851</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0900045c8357e602</td>\n",
       "      <td>XS1190850120</td>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0900045c8357f5f3</td>\n",
       "      <td>XS1190850559</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0900045c83457c8f</td>\n",
       "      <td>XS1190853496</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0900045c833cc8f5</td>\n",
       "      <td>XS1190855277</td>\n",
       "      <td>500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0900045c83561f55</td>\n",
       "      <td>XS1190857729</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0900045c83364a8e</td>\n",
       "      <td>XS1190859857</td>\n",
       "      <td>250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0900045c830019aa</td>\n",
       "      <td>XS1190871126</td>\n",
       "      <td>50000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0900045c83298c5a</td>\n",
       "      <td>XS1190873098</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0900045c82ef5890</td>\n",
       "      <td>XS1190876869</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0900045c82ec2887</td>\n",
       "      <td>XS1190879020</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0900045c831219a8</td>\n",
       "      <td>XS1190880119</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0900045c82f60697</td>\n",
       "      <td>XS1190885340</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0900045c82d8e212</td>\n",
       "      <td>XS1192625694</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>181 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               fileId          isin      actual predicted\n",
       "0    0900045c839f0d17  AT0000A0MPB1         200         0\n",
       "1    0900045c83f01550  AU3FN0030839      100000         0\n",
       "2    0900045c83f8dc8e  AU3FN0030912        2000         0\n",
       "3    0900045c838bd72f  CH0254068866      100000         0\n",
       "4    0900045c83bae03d  CH0254071365        5000         0\n",
       "5    0900045c83bba3a8  CH0254071431      150000         0\n",
       "6    0900045c8398c206  CH0266691317        2000         0\n",
       "7    0900045c83f93596  CH0266712170       20000         0\n",
       "8    0900045c83855f37  CH0266718706        2000         0\n",
       "9    0900045c83f085ae  CH0266720322           0    100000\n",
       "10   0900045c831515d2  CH0266741278      100000         0\n",
       "11   0900045c8303f377  CH0266741732        2000         0\n",
       "12   0900045c840e26ab  CH0266742722           0    100000\n",
       "13   0900045c8311fa5c  CH0266743654      100000         0\n",
       "14   0900045c83516e3f  CH0266744942       10000         0\n",
       "15   0900045c83070e18  CH0273393162      200000         0\n",
       "16   0900045c831a1d0c  CH0273396603      100000         0\n",
       "17   0900045c831dd9d9  CH0274763686  1000000000         0\n",
       "18   0900045c830bf519  CH0277773989      100000         0\n",
       "19   0900045c83cde80b  CH0279920281      200000         0\n",
       "20   0900045c833eec1d  CH0279923616      100000         0\n",
       "21   0900045c834e20c1  CH0279924770      100000         0\n",
       "22   0900045c8314646b  CH0281105046        2000         0\n",
       "23   0900045c835cfdd0  CH0283709548        2000         0\n",
       "24   0900045c8359dcd9  CH0283709647      100000         0\n",
       "25   0900045c833b4163  CH0283710165      100000         0\n",
       "26   0900045c83790c62  CH0283715131      200000         0\n",
       "27   0900045c8398bb45  CH0283716741        2000         0\n",
       "28   0900045c8342507e  CH0284402853      100000         0\n",
       "29   0900045c83470599  CH0285315823      200000         0\n",
       "..                ...           ...         ...       ...\n",
       "151  0900045c82b8065f  XS1140692481        2000         0\n",
       "152  0900045c82c0b0ba  XS1141959368      250000         0\n",
       "153  0900045c83e252ef  XS1143231527      200000         0\n",
       "154  0900045c8318b8d6  XS1143236831      250000         0\n",
       "155  0900045c82d17828  XS1167051256        1000         0\n",
       "156  0900045c82dd35f3  XS1173869022      100000         0\n",
       "157  0900045c82cd0a3b  XS1173870111        2000         0\n",
       "158  0900045c82f0734f  XS1182591575      200000         0\n",
       "159  0900045c831ac2a2  XS1184956834        2000         0\n",
       "160  0900045c8326be41  XS1184959770      200000         0\n",
       "161  0900045c82d837d5  XS1189246140  2020000000         0\n",
       "162  0900045c8394a39d  XS1190837960      100000         0\n",
       "163  0900045c839c90d3  XS1190839073      500000         0\n",
       "164  0900045c838a58b8  XS1190839156        2000         0\n",
       "165  0900045c8389117d  XS1190839743      100000         0\n",
       "166  0900045c83889f1c  XS1190840162        2000         0\n",
       "167  0900045c8364ac0d  XS1190843851           0    100000\n",
       "168  0900045c8357e602  XS1190850120     1000000         0\n",
       "169  0900045c8357f5f3  XS1190850559      200000         0\n",
       "170  0900045c83457c8f  XS1190853496         200         0\n",
       "171  0900045c833cc8f5  XS1190855277      500000         0\n",
       "172  0900045c83561f55  XS1190857729        2000         0\n",
       "173  0900045c83364a8e  XS1190859857      250000         0\n",
       "174  0900045c830019aa  XS1190871126       50000         0\n",
       "175  0900045c83298c5a  XS1190873098        2000         0\n",
       "176  0900045c82ef5890  XS1190876869        2000         0\n",
       "177  0900045c82ec2887  XS1190879020       10000         0\n",
       "178  0900045c831219a8  XS1190880119      100000         0\n",
       "179  0900045c82f60697  XS1190885340        2000         0\n",
       "180  0900045c82d8e212  XS1192625694           0    100000\n",
       "\n",
       "[181 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compile_mismatched_data_summary(y_test, y_test_pred, labels_path, doc_isin_path, label_name):\n",
    "    labels = pd.read_csv(labels_path, header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "    data_labeled_df = pd.DataFrame(labels[['ISIN', label_name]])\n",
    "    data_labeled_df.set_index(['ISIN'])\n",
    "    data_labeled_df = data_labeled_df.drop_duplicates()\n",
    "    data_labeled_df.sort_values('ISIN', axis=0, inplace=True)\n",
    "\n",
    "    idx = np.where(y_test != y_test_pred)\n",
    "    mismatched_isins = data_labeled_df.as_matrix()[idx]\n",
    "\n",
    "    all_labels_df = pd.read_csv(doc_isin_path, header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "    all_labels_df.set_index(['ISIN'])\n",
    "    all_labels_df.sort_values('ISIN', axis=0, inplace=True)\n",
    "\n",
    "    all_labels = all_labels_df.as_matrix(columns=['DOCID', 'ISIN'])\n",
    "\n",
    "    file_ids = []\n",
    "    for isin in mismatched_isins[:,0]:\n",
    "        idxx = all_labels_df['ISIN'] == isin\n",
    "        file_id = all_labels_df['DOCID'][idxx].values[0]\n",
    "        file_ids.append(file_id)\n",
    "        \n",
    "    summary = np.c_[np.array(file_ids), mismatched_isins[:,0], y_test[idx], y_test_pred[idx]]\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary, columns=['fileId', 'isin', 'actual', 'predicted'])\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "compile_mismatched_data_summary(y_test, y_test_pred, datafolder + 'outcome/ISIN_train.csv', \\\n",
    "                                datafolder + 'docID/docid_train.csv', 'MIN.TRAD.AMT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0900045c833fd034', 'XS0461332347', 'USD', 2000.0, 1000.0, 'N',\n",
       "        'USUB', 'N', 'USD', 'DEUTSCHE BANK AG, GREAT WINCHE', 'LONDON',\n",
       "        'GERMANY', nan, nan, nan]], dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv( datafolder + \"labels.csv\", header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "get_labels_for_isin(labels,'XS0461332347')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import cross_validation\n",
    "\n",
    "dataset, labelset = load_and_sort_training_data('min_labeled_docs_by_isin.pickle')\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(dataset, labelset,\\\n",
    "                                                                     test_size=0.3, random_state=53)\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer()), \\\n",
    "                     ('tfidf', TfidfTransformer()), \\\n",
    "                     ('clf', SGDClassifier())])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.0001, 0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__n_iter': (10, 50, 80)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minimum_amount_pipeline.pkl',\n",
       " 'minimum_amount_pipeline.pkl_01.npy',\n",
       " 'minimum_amount_pipeline.pkl_02.npy',\n",
       " 'minimum_amount_pipeline.pkl_03.npy',\n",
       " 'minimum_amount_pipeline.pkl_04.npy',\n",
       " 'minimum_amount_pipeline.pkl_05.npy',\n",
       " 'minimum_amount_pipeline.pkl_06.npy']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "dataset, labelset = load_and_sort_training_data('min_labeled_docs_by_isin.pickle')\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \\\n",
    "                     ('tfidf', TfidfTransformer()), \\\n",
    "                     ('clf', SGDClassifier())])\n",
    "\n",
    "text_clf = text_clf.set_params(clf__alpha = 1e-05, \\\n",
    "                               clf__n_iter = 50, \\\n",
    "                               clf__penalty = 'l2', \\\n",
    "                               clf__n_jobs = -1, \\\n",
    "                               tfidf__norm = 'l2', \\\n",
    "                               tfidf__use_idf = True, \\\n",
    "                               vect__max_df = 1.0, \\\n",
    "                               vect__ngram_range = (1, 2))\n",
    "\n",
    "text_clf = text_clf.fit(dataset, labelset)\n",
    "\n",
    "joblib.dump(text_clf, 'minimum_amount_pipeline.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCID</th>\n",
       "      <th>ISIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0900045c80a74f74</td>\n",
       "      <td>US29269MAA45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0900045c80b066db</td>\n",
       "      <td>XS0480504256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0900045c80c35015</td>\n",
       "      <td>XS0532879300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0900045c80c883a0</td>\n",
       "      <td>US40430CLJ61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0900045c80cd2703</td>\n",
       "      <td>XS0673671623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DOCID          ISIN\n",
       "0  0900045c80a74f74  US29269MAA45\n",
       "1  0900045c80b066db  XS0480504256\n",
       "2  0900045c80c35015  XS0532879300\n",
       "3  0900045c80c883a0  US40430CLJ61\n",
       "4  0900045c80cd2703  XS0673671623"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "unlabeled_isins = pd.read_csv('../int_test/docID/docid_int_test.csv', header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "unlabeled_isins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File b'currencyExtraction/optCur_int.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2109e63c44aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabeled_currencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'currencyExtraction/optCur_int.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ISIN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OPS.CURR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthousands\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabeled_currencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ciprian/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    496\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ciprian/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ciprian/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ciprian/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ciprian/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3246)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6111)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File b'currencyExtraction/optCur_int.csv' does not exist"
     ]
    }
   ],
   "source": [
    "labeled_currencies = pd.read_csv('currencyExtraction/optCur_int.csv', header=None, names=['ISIN', 'OPS.CURR'], sep=\",\", quoting=1, thousands=\",\")\n",
    "labeled_currencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Finished grouping file contents indexed by ISIN\n",
      "Saved file contents indexed by ISIN to: unlabeled_amt_docs_by_isin.pickle\n",
      "Processed 424 files in 24.544s\n",
      "OK!\n"
     ]
    }
   ],
   "source": [
    "group_docs_by_isin(html1_folder, labeled_currencies, unlabeled_isins, pickle_results=True, pickle_file='unlabeled_amt_docs_by_isin.pickle')\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "unlabeled_data = pickle.load(open('unlabeled_amt_docs_by_isin.pickle', 'rb'))\n",
    "data_by_isin = [[k, v] for k, v in unlabeled_data.items()]\n",
    "data_by_isin_df = pd.DataFrame(data_by_isin, columns = ['isin','content'])\n",
    "data_by_isin_df.sort_values('isin', axis=0, inplace=True)\n",
    "  \n",
    "unlabeled_dataset = data_by_isin_df.as_matrix(columns=['content']).flatten()\n",
    "isins = data_by_isin_df.as_matrix(columns=['isin']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "#load the classifier\n",
    "clf = joblib.load('minimum_amount_pipeline.pkl')\n",
    "multiple_amounts = clf.predict(unlabeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "results_df = pd.DataFrame(np.vstack((isins, multiple_amounts)).T, columns=['ISIN', 'MIN.TRAD.AMT'])\n",
    "\n",
    "results_df.to_csv('final_minTrad.csv', index=False, header=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
