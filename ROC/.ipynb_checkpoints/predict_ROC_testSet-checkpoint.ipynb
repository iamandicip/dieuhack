{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_classifier_ROC_trainSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15/06/16 by greghor\n",
    "\n",
    "\n",
    "on commence par regarder les documents ou l on observe le mot center\n",
    "utilise les fichiers .html\n",
    "integre corrections typographiques pour coller à la solution\n",
    "integre ROC à partir des currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PythonMagick package is missing, get_text_adv function can not be used \n",
      "The PyPDF2 package is missing, get_text_adv function can not be used \n",
      "The html2text package is missing, get_text_adv and get_text functions can not be used.\n",
      "Please consider the instalation of this package !\n"
     ]
    }
   ],
   "source": [
    "# SYSTEM\n",
    "from os import listdir\n",
    "from PyCommonFun import *\n",
    "\n",
    "#NLTK\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.metrics import *\n",
    "\n",
    "#GENERAL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "# data from Albert\n",
    "from code.dataExtract import data\n",
    "import code.dataExtract as de\n",
    "from code import Text_reader as tr\n",
    "# Read the data\n",
    "d = data(folder='../../train/') # training set with all the labels\n",
    "dataPath='../../train/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fonctions used in the notebook\n",
    "\n",
    "def getContext(s,pattern,context=list(),width=100):\n",
    "    \n",
    "    \n",
    "    loc=s.find(pattern)\n",
    "    \n",
    "    if loc == -1:\n",
    "        return context\n",
    "    elif loc-width < 0:\n",
    "        context+=s[:loc+width]\n",
    "        return getContext(s[loc+len(pattern):],pattern,context,width)\n",
    "    elif loc + width > len(s):\n",
    "        context+=s[loc-width:]\n",
    "        return getContext(s[loc+len(pattern):],pattern,context,width)\n",
    "    else:\n",
    "        context+=s[loc-width:loc+width]\n",
    "        return getContext(s[loc+len(pattern):],pattern,context,width)\n",
    "\n",
    "\n",
    "    \n",
    "def useSolutionTypo(s):\n",
    "    s=s.lower()\n",
    "    if s==\"sao paulo\":\n",
    "        s=\"sao-paulo\"\n",
    "    elif s=='target 1' or s =='target 2' or s == 'target' or s=='target1':\n",
    "        s='target holidays'\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def document_from_hmtl(doc_path):\n",
    "    with open(doc_path) as html_file:\n",
    "        full_text = html_file.read()\n",
    "        body = re.findall(r'<body[^>]*?>(.*?)</body>', full_text)\n",
    "        cleantext = re.sub('<\\/?span[^>]*>', ' ', body[0])\n",
    "        clean_text = re.sub('<.*?>', ' ', cleantext)\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = re.sub('0.01', '1', clean_text)\n",
    "        clean_text = re.sub(',', '', clean_text)\n",
    "        clean_text = re.sub(r'[^a-z0-9]', ' ', clean_text)\n",
    "        clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "        return clean_text\n",
    "\n",
    "def removeUnwantedCity(city, unwantedCity):\n",
    "    #\n",
    "    assert isinstance(city,list) or isinstance(city,np.ndarray), \"city should be an np.array or a list\"\n",
    "    assert isinstance(unwantedCity,list),  \"unwanted city should be a list\"\n",
    "        \n",
    "    for gi in range(len(city)):\n",
    "        for unwanted in unwantedCity:\n",
    "            if '|' + unwanted in city[gi]:\n",
    "                city[gi]=city[gi].replace('|'+ unwanted,'')\n",
    "            elif unwanted + '|' in city[gi]:\n",
    "                city[gi]=city[gi].replace(unwanted + '|','')\n",
    "            elif unwanted in city[gi]:\n",
    "                city[gi]=city[gi].replace(unwanted,'')\n",
    "    return city\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the solution\n",
    "ROCTmp=pd.read_csv('ROC_fullList.csv',delimiter=\";\")\n",
    "cities_groundTruth=np.load('../currencyExtraction/predicted_opCurr_dict.npy').item()\n",
    "ROC_list=['target 1','target 2','target'] # add target 1 and target 2\n",
    "# lowerise ROC and remove ponctuation\n",
    "for roc in ROCTmp['CIT_NM'].values:\n",
    "    roc=str(roc).lower().translate(None, string.punctuation)\n",
    "    ROC_list.append(roc.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# link roc and operationnal curency\n",
    "opCurrency=np.load('../currencyExtraction/predicted_opCurr_dict.npy').item()\n",
    "opCurr_df=pd.DataFrame(opCurrency.items(),columns=['ISIN','OpCurrency'])\n",
    "link_roc_opCurr=pd.read_csv('currency_ROC_fullList.csv',delimiter=\";\")\n",
    "# merge with labels on operationnal currency\n",
    "roc_fromCurrency=pd.merge(left=link_roc_opCurr,right=opCurr_df,left_on='Currency Code', right_on='OpCurrency')\n",
    "# keep only currency code and default ROC columns\n",
    "col=[\"ISIN\",\"OpCurrency\",\"Default ROC\",]\n",
    "roc_fromCurrency=roc_fromCurrency[col]\n",
    "roc_fromCurrency.rename(columns={'Default ROC':'currency_ROC'},inplace=True)\n",
    "len(roc_fromCurrency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0183284457478\n",
      "0.0366568914956\n",
      "0.0549853372434\n",
      "0.0733137829912\n",
      "0.091642228739\n",
      "0.109970674487\n",
      "0.128299120235\n",
      "0.146627565982\n",
      "0.16495601173\n",
      "0.183284457478\n",
      "0.201612903226\n",
      "0.219941348974\n",
      "0.238269794721\n",
      "0.256598240469\n",
      "0.274926686217\n",
      "0.293255131965\n",
      "0.311583577713\n",
      "0.32991202346\n",
      "0.348240469208\n",
      "0.366568914956\n",
      "0.384897360704\n",
      "0.403225806452\n",
      "0.421554252199\n",
      "0.439882697947\n",
      "0.458211143695\n",
      "0.476539589443\n",
      "0.494868035191\n",
      "0.513196480938\n",
      "0.531524926686\n",
      "0.549853372434\n",
      "0.568181818182\n",
      "0.58651026393\n",
      "0.604838709677\n",
      "0.623167155425\n",
      "0.641495601173\n",
      "0.659824046921\n",
      "0.678152492669\n",
      "0.696480938416\n",
      "0.714809384164\n",
      "0.733137829912\n",
      "0.75146627566\n",
      "0.769794721408\n",
      "0.788123167155\n",
      "0.806451612903\n",
      "0.824780058651\n",
      "0.843108504399\n",
      "0.861436950147\n",
      "0.879765395894"
     ]
    }
   ],
   "source": [
    "gf=0\n",
    "\n",
    "\n",
    "context_pat=dict()\n",
    "text=dict()\n",
    "htmlFiles=tr.get_files(dataPath + '/html')\n",
    "\n",
    "\n",
    "\n",
    "for isin in d.docid.keys()[:]:\n",
    "        \n",
    "    text[isin]=document_from_hmtl(htmlFiles[d.docid[isin][0]])\n",
    "    context_pat[isin]=''\n",
    "\n",
    "    pattern=[\"Business Day Centre\",\"Financial Centre\",\"Business Centre\",\"Currency Business Day\",'Business Day']\n",
    "    \n",
    "    ## recupere le contexte si patterns, prend l ensemble du text sinon\n",
    "    \n",
    "    context_pat[isin]+=text[isin]\n",
    "    \n",
    "    \"\"\"  \n",
    "    for pat in pattern:\n",
    "        context_pat[isin]+=getContext(text[isin],pat.lower(),'',150)\n",
    "        # il faut impérativement passer, un troisieme argument vide pour que le code \n",
    "        # fonctionne mais je ne comprends pas bien pourquoi\n",
    "            \n",
    "    if len(context_pat[isin])==0:\n",
    "        context_pat[isin]+=text[isin]\"\"\"\n",
    "            \n",
    "    gf+=1\n",
    "    progression=float(gf)/len(d.docid.keys())*100\n",
    "    if gf%100==0:\n",
    "        print float(gf)/len(d.docid.keys())\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(PyFind(context_pat.values(),lambda x : len(x)==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data for classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the labels and merge with context\n",
    "citiesGT=pd.read_csv('citiesGreg.csv')\n",
    "context_df=pd.DataFrame(context_pat.items(),columns=['isin','context'])\n",
    "training_df=pd.merge(context_df,citiesGT,on='isin')\n",
    "training_df=training_df[['isin','context','citiesName']]\n",
    "print training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the list and feed the trainer\n",
    "\n",
    "context=training_df.as_matrix(columns=['context']).ravel()\n",
    "# lowerise, and replace pipe by space for the labels\n",
    "roc=[roc.lower().replace('|',' ') for roc in training_df['citiesName'].values]\n",
    "roc=np.asarray(roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(context, roc,\\\n",
    "                                                                     test_size=0.3, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scikit impory\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### trainac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train \n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \\\n",
    "                     ('tfidf', TfidfTransformer()), \\\n",
    "                     ('clf', SGDClassifier(average=True))])\n",
    "\n",
    "text_clf = text_clf.set_params(clf__alpha = 1e-04, \\\n",
    "                               clf__n_iter = 100, \\\n",
    "                               clf__penalty = 'l2', \\\n",
    "                               clf__n_jobs=-1,\\\n",
    "                               tfidf__norm = 'l2', \\\n",
    "                               tfidf__use_idf = False, \\\n",
    "                               vect__max_df = 0.4, \\\n",
    "                               vect__ngram_range = (1, 2))\n",
    "\n",
    "text_clf = text_clf.fit(X_train.ravel(), y_train.ravel())\n",
    "\n",
    "y_train_pred = text_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(np.mean(y_train == y_train_pred))\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = text_clf.predict(X_test)\n",
    "print(text_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    joblib.dump(text_clf, 'sgd_model/currency_100Trainpipeline.pkl') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
