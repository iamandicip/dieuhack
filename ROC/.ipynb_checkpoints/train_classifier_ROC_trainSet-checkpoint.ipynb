{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_classifier_ROC_trainSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15/06/16 by greghor\n",
    "\n",
    "\n",
    "on commence par regarder les documents ou l on observe le mot center\n",
    "utilise les fichiers .html\n",
    "integre corrections typographiques pour coller à la solution\n",
    "integre ROC à partir des currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PythonMagick package is missing, get_text_adv function can not be used \n",
      "The PyPDF2 package is missing, get_text_adv function can not be used \n",
      "The html2text package is missing, get_text_adv and get_text functions can not be used.\n",
      "Please consider the instalation of this package !\n"
     ]
    }
   ],
   "source": [
    "# SYSTEM\n",
    "from os import listdir\n",
    "from PyCommonFun import *\n",
    "\n",
    "#NLTK\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.metrics import *\n",
    "\n",
    "#GENERAL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "# data from Albert\n",
    "from code.dataExtract import data\n",
    "import code.dataExtract as de\n",
    "from code import Text_reader as tr\n",
    "# Read the data\n",
    "d = data(folder='../../train/') # training set with all the labels\n",
    "dataPath='../../train/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fonctions used in the notebook\n",
    "\n",
    "def getContext(s,pattern,context=list(),width=100):\n",
    "    \n",
    "    \n",
    "    loc=s.find(pattern)\n",
    "    \n",
    "    if loc == -1:\n",
    "        return context\n",
    "    elif loc-width < 0:\n",
    "        context+=s[:loc+width]\n",
    "        return getContext(s[loc+len(pattern):],pattern,context,width)\n",
    "    elif loc + width > len(s):\n",
    "        context+=s[loc-width:]\n",
    "        return getContext(s[loc+len(pattern):],pattern,context,width)\n",
    "    else:\n",
    "        context+=s[loc-width:loc+width]\n",
    "        return getContext(s[loc+len(pattern):],pattern,context,width)\n",
    "\n",
    "\n",
    "    \n",
    "def useSolutionTypo(s):\n",
    "    s=s.lower()\n",
    "    if s==\"sao paulo\":\n",
    "        s=\"sao-paulo\"\n",
    "    elif s=='target 1' or s =='target 2' or s == 'target' or s=='target1':\n",
    "        s='target holidays'\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def document_from_hmtl(doc_path):\n",
    "    with open(doc_path) as html_file:\n",
    "        full_text = html_file.read()\n",
    "        body = re.findall(r'<body[^>]*?>(.*?)</body>', full_text)\n",
    "        cleantext = re.sub('<\\/?span[^>]*>', ' ', body[0])\n",
    "        clean_text = re.sub('<.*?>', ' ', cleantext)\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = re.sub('0.01', '1', clean_text)\n",
    "        clean_text = re.sub(',', '', clean_text)\n",
    "        clean_text = re.sub(r'[^a-z0-9]', ' ', clean_text)\n",
    "        clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "        return clean_text\n",
    "\n",
    "def removeUnwantedCity(city, unwantedCity):\n",
    "    #\n",
    "    assert isinstance(city,list) or isinstance(city,np.ndarray), \"city should be an np.array or a list\"\n",
    "    assert isinstance(unwantedCity,list),  \"unwanted city should be a list\"\n",
    "        \n",
    "    for gi in range(len(city)):\n",
    "        for unwanted in unwantedCity:\n",
    "            if '|' + unwanted in city[gi]:\n",
    "                city[gi]=city[gi].replace('|'+ unwanted,'')\n",
    "            elif unwanted + '|' in city[gi]:\n",
    "                city[gi]=city[gi].replace(unwanted + '|','')\n",
    "            elif unwanted in city[gi]:\n",
    "                city[gi]=city[gi].replace(unwanted,'')\n",
    "    return city\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the solution\n",
    "ROCTmp=pd.read_csv('ROC_fullList.csv',delimiter=\";\")\n",
    "cities_groundTruth=np.load('../currencyExtraction/predicted_opCurr_dict.npy').item()\n",
    "ROC_list=['target 1','target 2','target'] # add target 1 and target 2\n",
    "# lowerise ROC and remove ponctuation\n",
    "for roc in ROCTmp['CIT_NM'].values:\n",
    "    roc=str(roc).lower().translate(None, string.punctuation)\n",
    "    ROC_list.append(roc.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# link roc and operationnal curency\n",
    "opCurrency=np.load('../currencyExtraction/predicted_opCurr_dict.npy').item()\n",
    "opCurr_df=pd.DataFrame(opCurrency.items(),columns=['ISIN','OpCurrency'])\n",
    "link_roc_opCurr=pd.read_csv('currency_ROC_fullList.csv',delimiter=\";\")\n",
    "# merge with labels on operationnal currency\n",
    "roc_fromCurrency=pd.merge(left=link_roc_opCurr,right=opCurr_df,left_on='Currency Code', right_on='OpCurrency')\n",
    "# keep only currency code and default ROC columns\n",
    "col=[\"ISIN\",\"OpCurrency\",\"Default ROC\",]\n",
    "roc_fromCurrency=roc_fromCurrency[col]\n",
    "roc_fromCurrency.rename(columns={'Default ROC':'currency_ROC'},inplace=True)\n",
    "len(roc_fromCurrency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "gf=0\n",
    "\n",
    "\n",
    "context_pat=dict()\n",
    "text=dict()\n",
    "htmlFiles=tr.get_files(dataPath + '/html')\n",
    "\n",
    "\n",
    "\n",
    "for isin in d.docid.keys()[:]:\n",
    "        \n",
    "    text[isin]=document_from_hmtl(htmlFiles[d.docid[isin][0]])\n",
    "    context_pat[isin]=''\n",
    "\n",
    "    pattern=[\"Business Day Centre\",\"Financial Centre\",\"Business Centre\",\"Currency Business Day\",'Business Day']\n",
    "    \n",
    "    ## recupere le contexte si patterns, prend l ensemble du text sinon\n",
    "    \n",
    "    context_pat[isin]+=text[isin]\n",
    "    \n",
    "    \"\"\"  \n",
    "    for pat in pattern:\n",
    "        context_pat[isin]+=getContext(text[isin],pat.lower(),'',150)\n",
    "        # il faut impérativement passer, un troisieme argument vide pour que le code \n",
    "        # fonctionne mais je ne comprends pas bien pourquoi\n",
    "            \n",
    "    if len(context_pat[isin])==0:\n",
    "        context_pat[isin]+=text[isin]\"\"\"\n",
    "            \n",
    "    gf+=1\n",
    "    progression=float(gf)/len(d.docid.keys())*100\n",
    "    if gf%100==0:\n",
    "        print float(gf)/len(d.docid.keys())\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PyFind(context_pat.values(),lambda x : len(x)==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data for classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           isin                                            context  \\\n",
      "0  XS1313800895   no prospectus is required in accordance with ...   \n",
      "1  XS1267246327   pricing supplement dated december 10 2015 exe...   \n",
      "2  XS1267238654   pricing supplement dated december 9 2015 gold...   \n",
      "3  XS1293107071   final terms dated 16 december 2015 bnp pariba...   \n",
      "4  XS1326548325   no prospectus is required in accordance with ...   \n",
      "\n",
      "                 citiesName  \n",
      "0              LONDON|TOKYO  \n",
      "1  NEW YORK|TARGET HOLIDAYS  \n",
      "2                  NEW YORK  \n",
      "3           TARGET HOLIDAYS  \n",
      "4                 HONG KONG  \n"
     ]
    }
   ],
   "source": [
    "# get the labels and merge with context\n",
    "citiesGT=pd.read_csv('citiesGreg.csv')\n",
    "context_df=pd.DataFrame(context_pat.items(),columns=['isin','context'])\n",
    "training_df=pd.merge(context_df,citiesGT,on='isin')\n",
    "training_df=training_df[['isin','context','citiesName']]\n",
    "print training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the list and feed the trainer\n",
    "\n",
    "context=training_df.as_matrix(columns=['context']).ravel()\n",
    "# lowerise, and replace pipe by space for the labels\n",
    "roc=[roc.lower().replace('|',' ') for roc in training_df['citiesName'].values]\n",
    "roc=np.asarray(roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['london tokyo', 'new york target holidays', 'new york', ...,\n",
       "       'new york', 'hong kong', 'beijing hong kong'], \n",
       "      dtype='|S54')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(context, roc,\\\n",
    "                                                                     test_size=0.3, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \\\n",
    "                     ('tfidf', TfidfTransformer()), \\\n",
    "                     ('clf', SGDClassifier(average=True))])\n",
    "\n",
    "text_clf = text_clf.set_params(clf__alpha = 1e-04, \\\n",
    "                               clf__n_iter = 100, \\\n",
    "                               clf__penalty = 'l2', \\\n",
    "                               clf__n_jobs=-1,\\\n",
    "                               tfidf__norm = 'l2', \\\n",
    "                               tfidf__use_idf = False, \\\n",
    "                               vect__max_df = 0.4, \\\n",
    "                               vect__ngram_range = (1, 2))\n",
    "\n",
    "text_clf = text_clf.fit(X_train.ravel(), y_train.ravel())\n",
    "\n",
    "y_train_pred = text_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-951e1c1e619e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performing grid search...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.0001, 0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__n_jobs': -1,\n",
    "    'clf__n_iter': (10, 50, 80)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train[:100], y_train[:100])\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986180904523\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "auckland london new york wellington       1.00      1.00      1.00         1\n",
      "beijing hong kong       1.00      1.00      1.00         3\n",
      "beijing hong kong london new york toronto       1.00      1.00      1.00         1\n",
      "brasilia london new york tokyo       1.00      1.00      1.00         3\n",
      "   budapest       1.00      1.00      1.00         2\n",
      "caire new york       1.00      1.00      1.00         1\n",
      "  hong kong       0.95      1.00      0.97       136\n",
      "hong kong london new york       1.00      1.00      1.00         2\n",
      "hong kong london new york toronto       1.00      1.00      1.00         1\n",
      "hong kong london target holidays       1.00      1.00      1.00         1\n",
      "hong kong london tokyo       1.00      1.00      1.00         1\n",
      "hong kong new york       1.00      0.93      0.96        14\n",
      "hong kong sydney       1.00      1.00      1.00         1\n",
      "istanbul london new york target holidays tokyo       1.00      1.00      1.00         4\n",
      "istanbul london new york tokyo       1.00      1.00      1.00         3\n",
      "johannesburg london new york       1.00      1.00      1.00         1\n",
      "     london       1.00      1.00      1.00        11\n",
      "london luxembourg target holidays       1.00      1.00      1.00         1\n",
      "london mumbai new york singapore tokyo       1.00      1.00      1.00         1\n",
      "london new york       1.00      1.00      1.00        35\n",
      "london new york sao-paulo sydney target holidays tokyo       1.00      1.00      1.00         1\n",
      "london new york sao-paulo target holidays tokyo       1.00      1.00      1.00         4\n",
      "london new york sao-paulo tokyo       1.00      1.00      1.00        32\n",
      "london new york singapore       1.00      1.00      1.00         1\n",
      "london new york target holidays       1.00      1.00      1.00         2\n",
      "london new york target holidays tokyo       1.00      1.00      1.00         3\n",
      "london new york target holidays toronto       1.00      1.00      1.00         1\n",
      "london new york tokyo       1.00      0.97      0.99        35\n",
      "london oslo       1.00      1.00      1.00         1\n",
      "london sydney tokyo       1.00      1.00      1.00         9\n",
      "london target holidays       1.00      1.00      1.00        15\n",
      "london target holidays tokyo       1.00      1.00      1.00         4\n",
      "london tokyo       0.98      1.00      0.99       111\n",
      "     mexico       1.00      0.50      0.67         2\n",
      "moscou new york       1.00      1.00      1.00         1\n",
      "   new york       0.99      0.99      0.99       200\n",
      "new york sao-paulo       1.00      1.00      1.00         1\n",
      "new york singapore       1.00      1.00      1.00         3\n",
      "new york target holidays       1.00      1.00      1.00         6\n",
      "new york tokyo       1.00      1.00      1.00        12\n",
      "       oslo       1.00      1.00      1.00         1\n",
      "     prague       1.00      1.00      1.00         1\n",
      "  singapore       1.00      0.87      0.93        15\n",
      "     sydney       1.00      1.00      1.00         4\n",
      "sydney tokyo       1.00      1.00      1.00         2\n",
      "target holidays       1.00      0.93      0.97        46\n",
      "      tokyo       1.00      0.98      0.99        44\n",
      "     zurich       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       0.99      0.99      0.99       796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(np.mean(y_train == y_train_pred))\n",
    "\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792397660819\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = text_clf.predict(X_test)\n",
    "print(text_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merge with citiesGreg to check how many results are OK\n",
    "\n",
    "citiesGreg=pd.read_csv('citiesGreg.csv')\n",
    "joinedCities=pd.merge(citiesGreg,extractedROC_df,on='isin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check the compliance between ground truth and my solution\n",
    "indCheck=[]\n",
    "gval=0\n",
    "for multiCity in joinedCities['myCities'].values:\n",
    "    indCheckTmp=True\n",
    "    for city in multiCity.split('|'): # check that all extracted cities are in the ground Truth\n",
    "        indCheckTmp=indCheckTmp and (city in  joinedCities['citiesName'].values[gval])\n",
    "\n",
    "    # check that ground truth do not content additionnal city\n",
    "    if len(joinedCities['citiesName'].values[gval]) == len(multiCity):\n",
    "        indCheckTmp=indCheckTmp and True\n",
    "    else:\n",
    "        indCheckTmp=indCheckTmp and False\n",
    "    indCheck.append(indCheckTmp)\n",
    "    gval+=1\n",
    "joinedCities[\"check\"  ]=indCheck\n",
    "\n",
    "# reorder the columns for clarity\n",
    "col=['fileId',\n",
    " 'isin',\n",
    " 'citiesName', 'myCities','check',\n",
    " 'pattern']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_extractedCities=joinedCities[col]\n",
    "check_extractedCities.to_csv('check_extractedCities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(PyFind(check_extractedCities['check'].values, lambda x: x==True))/np.float(len(check_extractedCities['citiesName'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractedROC_sol['XS1266607586']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
