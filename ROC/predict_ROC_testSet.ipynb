{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict__ROC_trainSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15/06/16 by greghor\n",
    "\n",
    "\n",
    "on commence par regarder les documents ou l on observe le mot center\n",
    "utilise les fichiers .html\n",
    "integre corrections typographiques pour coller à la solution\n",
    "integre ROC à partir des currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PythonMagick package is missing, get_text_adv function can not be used \n",
      "The PyPDF2 package is missing, get_text_adv function can not be used \n",
      "The html2text package is missing, get_text_adv and get_text functions can not be used.\n",
      "Please consider the instalation of this package !\n"
     ]
    }
   ],
   "source": [
    "# SYSTEM\n",
    "from os import listdir\n",
    "from PyCommonFun import *\n",
    "\n",
    "#NLTK\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.metrics import *\n",
    "\n",
    "#GENERAL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "# data from Albert\n",
    "from code.dataExtract import data\n",
    "import code.dataExtract as de\n",
    "from code import Text_reader as tr\n",
    "# Read the data\n",
    "d = data(folder='../../int_test/') # training set with all the labels\n",
    "dataPath='../../int_test/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fonctions used in the notebook\n",
    "\n",
    "def getContext(s,pattern,context=list(),width=100):\n",
    "    \n",
    "    \n",
    "    loc=s.find(pattern)\n",
    "    \n",
    "    if loc == -1:\n",
    "        return context\n",
    "    elif loc-width < 0:\n",
    "        context+=s[:loc+width]\n",
    "        return getContext(s[loc+len(pattern):],pattern,context,width)\n",
    "    elif loc + width > len(s):\n",
    "        context+=s[loc-width:]\n",
    "        return getContext(s[loc+len(pattern):],pattern,context,width)\n",
    "    else:\n",
    "        context+=s[loc-width:loc+width]\n",
    "        return getContext(s[loc+len(pattern):],pattern,context,width)\n",
    "\n",
    "\n",
    "    \n",
    "def useSolutionTypo(s):\n",
    "    s=s.lower()\n",
    "    if s==\"sao paulo\":\n",
    "        s=\"sao-paulo\"\n",
    "    elif s=='target 1' or s =='target 2' or s == 'target' or s=='target1':\n",
    "        s='target holidays'\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def document_from_hmtl(doc_path):\n",
    "    with open(doc_path) as html_file:\n",
    "        full_text = html_file.read()\n",
    "        body = re.findall(r'<body[^>]*?>(.*?)</body>', full_text)\n",
    "        cleantext = re.sub('<\\/?span[^>]*>', ' ', body[0])\n",
    "        clean_text = re.sub('<.*?>', ' ', cleantext)\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = re.sub('0.01', '1', clean_text)\n",
    "        clean_text = re.sub(',', '', clean_text)\n",
    "        clean_text = re.sub(r'[^a-z0-9]', ' ', clean_text)\n",
    "        clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "        return clean_text\n",
    "\n",
    "def removeUnwantedCity(city, unwantedCity):\n",
    "    #\n",
    "    assert isinstance(city,list) or isinstance(city,np.ndarray), \"city should be an np.array or a list\"\n",
    "    assert isinstance(unwantedCity,list),  \"unwanted city should be a list\"\n",
    "        \n",
    "    for gi in range(len(city)):\n",
    "        for unwanted in unwantedCity:\n",
    "            if '|' + unwanted in city[gi]:\n",
    "                city[gi]=city[gi].replace('|'+ unwanted,'')\n",
    "            elif unwanted + '|' in city[gi]:\n",
    "                city[gi]=city[gi].replace(unwanted + '|','')\n",
    "            elif unwanted in city[gi]:\n",
    "                city[gi]=city[gi].replace(unwanted,'')\n",
    "    return city\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the solution\n",
    "ROCTmp=pd.read_csv('ROC_fullList.csv',delimiter=\";\")\n",
    "cities_groundTruth=np.load('../currencyExtraction/predicted_opCurr_dict.npy').item()\n",
    "ROC_list=['target 1','target 2','target'] # add target 1 and target 2\n",
    "# lowerise ROC and remove ponctuation\n",
    "for roc in ROCTmp['CIT_NM'].values:\n",
    "    roc=str(roc).lower().translate(None, string.punctuation)\n",
    "    ROC_list.append(roc.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# link roc and operationnal curency\n",
    "opCurrency=np.load('../currencyExtraction/predicted_opCurr_dict.npy').item()\n",
    "opCurr_df=pd.DataFrame(opCurrency.items(),columns=['ISIN','OpCurrency'])\n",
    "link_roc_opCurr=pd.read_csv('currency_ROC_fullList.csv',delimiter=\";\")\n",
    "# merge with labels on operationnal currency\n",
    "roc_fromCurrency=pd.merge(left=link_roc_opCurr,right=opCurr_df,left_on='Currency Code', right_on='OpCurrency')\n",
    "# keep only currency code and default ROC columns\n",
    "col=[\"ISIN\",\"OpCurrency\",\"Default ROC\",]\n",
    "roc_fromCurrency=roc_fromCurrency[col]\n",
    "roc_fromCurrency.rename(columns={'Default ROC':'currency_ROC'},inplace=True)\n",
    "len(roc_fromCurrency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.234741784038\n",
      "0.469483568075\n",
      "0.704225352113\n",
      "0.93896713615\n"
     ]
    }
   ],
   "source": [
    "gf=0\n",
    "\n",
    "\n",
    "context_pat=dict()\n",
    "text=dict()\n",
    "htmlFiles=tr.get_files(dataPath + '/html')\n",
    "\n",
    "\n",
    "\n",
    "for isin in d.docid.keys()[:]:\n",
    "        \n",
    "    text[isin]=document_from_hmtl(htmlFiles[d.docid[isin][0]])\n",
    "    context_pat[isin]=''\n",
    "\n",
    "    pattern=[\"Business Day Centre\",\"Financial Centre\",\"Business Centre\",\"Currency Business Day\",'Business Day']\n",
    "    \n",
    "    ## recupere le contexte si patterns, prend l ensemble du text sinon\n",
    "    \n",
    "    context_pat[isin]+=text[isin]\n",
    "    \n",
    "    \"\"\"  \n",
    "    for pat in pattern:\n",
    "        context_pat[isin]+=getContext(text[isin],pat.lower(),'',150)\n",
    "        # il faut impérativement passer, un troisieme argument vide pour que le code \n",
    "        # fonctionne mais je ne comprends pas bien pourquoi\n",
    "            \n",
    "    if len(context_pat[isin])==0:\n",
    "        context_pat[isin]+=text[isin]\"\"\"\n",
    "            \n",
    "    gf+=1\n",
    "    progression=float(gf)/len(d.docid.keys())*100\n",
    "    if gf%100==0:\n",
    "        print float(gf)/len(d.docid.keys())\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context_pat.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scikit impory\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "text_clf=joblib.load('sgd_model/roc_70Trainpipeline.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the conext and load the model\n",
    "context_df=pd.DataFrame(context_pat.items(),columns=['isin','context'])\n",
    "context=context_df.as_matrix(columns=['context']).ravel()\n",
    "# load the model\n",
    "text_clf=joblib.load('sgd_model/roc_70Trainpipeline.pkl')\n",
    "#predict\n",
    "predicted_roc=text_clf.predict(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted_roc_dict=dict()\n",
    "\n",
    "gi=0\n",
    "for isin in context_df['isin'].values:\n",
    "    predicted_roc_dict[isin]=predicted_roc[gi]\n",
    "    gi+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(predicted_roc_dict,open('predicted_roc_dict.pickle','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
