{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "datafolder = \"../train/\"\n",
    "assert path.isdir(datafolder)\n",
    "\n",
    "data1folder = '../final_test'\n",
    "assert path.isdir(data1folder)\n",
    "\n",
    "html1_folder = path.join(data1folder, \"html\")\n",
    "assert path.isdir(html1_folder)\n",
    "\n",
    "html_folder = path.join(datafolder, \"html\")\n",
    "assert path.isdir(html_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>ISSUER.NAME</th>\n",
       "      <th>ZCP.FL</th>\n",
       "      <th>MIN.TRAD.AMT</th>\n",
       "      <th>MLT.TRAD.AMT</th>\n",
       "      <th>OPS.CURR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARCBAS031621</td>\n",
       "      <td>CIUDAD DE BUENOS AIRES</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>ARS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT0000248448</td>\n",
       "      <td>UNICREDIT BANK AUSTRIA AG</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>100000</td>\n",
       "      <td>EUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT0000A0MPB1</td>\n",
       "      <td>BAWAG PSK BANK FUR ARBEIT UND WIRTSCHAFT UND O...</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>EUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT0000A17HH9</td>\n",
       "      <td>RAIFFEISEN CENTROBANK AG</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT0000A1HE76</td>\n",
       "      <td>RAIFFEISEN CENTROBANK AG</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>CZK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ISIN                                        ISSUER.NAME ZCP.FL  \\\n",
       "0  ARCBAS031621                             CIUDAD DE BUENOS AIRES      N   \n",
       "1  AT0000248448                          UNICREDIT BANK AUSTRIA AG      N   \n",
       "2  AT0000A0MPB1  BAWAG PSK BANK FUR ARBEIT UND WIRTSCHAFT UND O...      N   \n",
       "3  AT0000A17HH9                           RAIFFEISEN CENTROBANK AG      N   \n",
       "4  AT0000A1HE76                           RAIFFEISEN CENTROBANK AG      Y   \n",
       "\n",
       "   MIN.TRAD.AMT  MLT.TRAD.AMT OPS.CURR  \n",
       "0             0          1000      ARS  \n",
       "1             0        100000      EUR  \n",
       "2             0           100      EUR  \n",
       "3             0          1000      USD  \n",
       "4             0          1000      CZK  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_csv( datafolder + \"outcome/ISIN_train.csv\", header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "files_isins = pd.read_csv( datafolder + \"docID/docid_train.csv\", header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "\n",
    "def get_isin_for_file(labels, file_name, print_result = False):\n",
    "    isin = None\n",
    "    #function to get the isin from the labels, given a file name\n",
    "    file_id = file_name.split('_')[0]\n",
    "    idx = labels['DOCID'] == file_id\n",
    "    isins = labels['ISIN'][idx]\n",
    "    if isins.values: \n",
    "        isin = isins.values[0]\n",
    "    \n",
    "    if(print_result):\n",
    "        print(\"%s - %s\" % (file_name, isin))\n",
    "    return isin\n",
    "\n",
    "def get_labels_for_isin(labels, isin):\n",
    "    idx = labels['ISIN'] == isin\n",
    "    return labels[:][idx].values\n",
    "\n",
    "def get_label_value_for_isin(labels, isin, attr_name):\n",
    "    #function to return the value of the label for a given isin\n",
    "    idx = labels['ISIN'] == isin\n",
    "    return labels[attr_name][idx].values[0]\n",
    "\n",
    "def save_file(isin, text):\n",
    "    f = open(isin + '.txt', 'w')\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "\n",
    "def document_to_words(doc_path):\n",
    "    with open(doc_path, 'r', encoding='utf8') as html_file:\n",
    "        full_text = html_file.read()\n",
    "        body = re.findall(r'<body[^>]*?>(.*?)</body>', full_text)\n",
    "        cleantext = re.sub('<\\/?span[^>]*>', ' ', body[0])\n",
    "        clean_text = re.sub('<.*?>', ' ', cleantext)\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = re.sub('0.01', '1', clean_text)\n",
    "        clean_text = re.sub(',', '', clean_text)\n",
    "        clean_text = re.sub(r'[^a-z0-9]', ' ', clean_text)\n",
    "        clean_text = re.sub('\\s+', ' ', clean_text)\n",
    "        words = nltk.word_tokenize(clean_text)\n",
    "        stops = set(stopwords.words('english'))\n",
    "        relevant_words = [w for w in words if w not in stops and w != '' and w != ' ']\n",
    "        return relevant_words\n",
    "\n",
    "keywords_contexts = [['nominal', 'amount'], \\\n",
    "                     ['denomination'], \\\n",
    "                     ['denominations'], \\\n",
    "                     ['notional', 'amount', 'certificate'], \\\n",
    "                     ['specified', 'denomination'], \\\n",
    "                     ['specified', 'denominations'],\n",
    "                     ['payable', 'security', 'nominal', 'amount'],\n",
    "                     ['increasing', 'multiples']\n",
    "                    ]\n",
    "\n",
    "def contains(small_list, big_list):\n",
    "    for word in small_list:\n",
    "        if word not in big_list:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def keywords_in_context(keywords, context):\n",
    "    for keywords_list in keywords:\n",
    "        if contains(keywords_list, context):\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def sort_contexts(contexts):\n",
    "    c = sorted(contexts, key=len)\n",
    "    c.reverse()\n",
    "    return c\n",
    "\n",
    "keywords_contexts = sort_contexts(keywords_contexts)\n",
    "\n",
    "def extract_relevant_info(words, op_curr, window_before, window_after):\n",
    "#     print('extracting info for : %s - %s', (op_curr, words))\n",
    "\n",
    "    AMOUNT_REGEX = r'\\d{1,12}'\n",
    "    CURR_AMOUNT_REGEX = r'[a-z]{3}\\d{1,12}'\n",
    "    \n",
    "    relevant_info = []\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "\n",
    "        if word == op_curr and i + window_after < len(words):\n",
    "            #retain look_around words around the keyword\n",
    "            context_before = words[i - window_before : i]\n",
    "            \n",
    "            keywords_present = keywords_in_context(keywords_contexts, context_before)\n",
    "            \n",
    "            if keywords_present:\n",
    "                context_after = words[i : i + window_after]\n",
    "                context = context_before + context_after\n",
    "                relevant_info += context\n",
    "\n",
    "        elif word.startswith(op_curr) and re.match(CURR_AMOUNT_REGEX, word):\n",
    "            context_before = words[i - window_before : i]\n",
    "            context_after = words[i : i + window_after]\n",
    "            matches = re.findall(AMOUNT_REGEX, word)\n",
    "            context = context_before + [op_curr] + matches + context_after\n",
    "\n",
    "            relevant_info += context\n",
    "            \n",
    "    return ' '.join(relevant_info)\n",
    "\n",
    "inspect = []\n",
    "\n",
    "def group_docs_by_isin(html_folder, labels, files_isins, pickle_results=False, pickle_file='docs_by_isin.pickle'):\n",
    "    t0 = time()\n",
    "    \n",
    "    contents_by_isin = {}\n",
    "    \n",
    "    print('Processing files')\n",
    "\n",
    "    for file_name in os.listdir(html_folder):\n",
    "        \n",
    "        print('.', end='')\n",
    "        \n",
    "        if file_name.endswith(\".html\"):\n",
    "\n",
    "            #first find the isin corresponding to this file\n",
    "            isin = get_isin_for_file(files_isins, file_name)\n",
    "\n",
    "            if(isin is not None):\n",
    "\n",
    "                #found the isin to associate the document with\n",
    "                words = document_to_words(path.join(html_folder, file_name))\n",
    "                \n",
    "                op_curr = get_label_value_for_isin(labels, isin, 'OPS.CURR')\n",
    "                \n",
    "                window_before = 8\n",
    "                window_after = 8\n",
    "                \n",
    "                if isin in inspect:\n",
    "                    save_file(isin, ' '.join(words))\n",
    "                \n",
    "                file_content = extract_relevant_info(words, op_curr.lower(), window_before, window_after)\n",
    "#                 file_content = ' '.join(words)\n",
    "\n",
    "                try:\n",
    "                    #if there is already existing data for this isin, append the new data\n",
    "                    existing_isin_data = contents_by_isin[isin]\n",
    "                    contents_by_isin[isin] = existing_isin_data + file_content\n",
    "\n",
    "                except KeyError:\n",
    "                    contents_by_isin[isin] = file_content\n",
    "    \n",
    "    print('\\nFinished grouping file contents indexed by ISIN')\n",
    "    \n",
    "    if pickle_results:\n",
    "        pickle.dump(contents_by_isin, open(pickle_file, 'wb'))\n",
    "        \n",
    "        print('Saved file contents indexed by ISIN to:', pickle_file)\n",
    "        \n",
    "    print('Processed %d files in %0.3fs' % (len(contents_by_isin.keys()), (time() - t0)))\n",
    "        \n",
    "    return contents_by_isin\n",
    "\n",
    "def load_and_sort_training_data(file_name):\n",
    "    data = pickle.load(open(file_name, 'rb'))\n",
    "    data_by_isin = [[k, v] for k, v in data.items()]\n",
    "    data_by_isin_df = pd.DataFrame(data_by_isin, columns = ['isin','content'])\n",
    "    data_by_isin_df.sort_values('isin', axis=0, inplace=True)\n",
    "  \n",
    "    dataset = data_by_isin_df.as_matrix(columns=['content'])\n",
    "    \n",
    "    labels = pd.read_csv( datafolder + 'outcome/ISIN_train.csv', header=0, sep=',', quoting=1, thousands=',')\n",
    "    data_labeled_df = pd.DataFrame(labels[['ISIN','MLT.TRAD.AMT']])\n",
    "    data_labeled_df.set_index(['ISIN'])\n",
    "    data_labeled_df = data_labeled_df.drop_duplicates()\n",
    "    data_labeled_df.sort_values('ISIN', axis=0, inplace=True)\n",
    "    labelset = data_labeled_df.as_matrix(columns=['MLT.TRAD.AMT'])\n",
    "    \n",
    "    return dataset.ravel(), labelset.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files\n",
      "..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Finished grouping file contents indexed by ISIN\n",
      "Saved file contents indexed by ISIN to: mult_labeled_docs_by_isin.pickle\n",
      "Processed 5456 files in 286.052s\n",
      "OK!\n"
     ]
    }
   ],
   "source": [
    "group_docs_by_isin(html_folder, labels, files_isins, pickle_results=True, pickle_file='mult_labeled_docs_by_isin.pickle')\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    }
   ],
   "source": [
    "dataset, labelset = load_and_sort_training_data('mult_labeled_docs_by_isin.pickle')\n",
    "print(dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "dataset, labelset = load_and_sort_training_data('mult_labeled_docs_by_isin.pickle')\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(dataset, labelset,\\\n",
    "                                                                     test_size=0.3, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier took 1.520s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \\\n",
    "                     ('tfidf', TfidfTransformer()), \\\n",
    "#                      ('pca', TruncatedSVD()), \\\n",
    "                     ('clf', SGDClassifier(average=True))\n",
    "#                      ('clf', RandomForestClassifier())\n",
    "                    ])\n",
    "\n",
    "\n",
    "text_clf = text_clf.set_params(\\\n",
    "                               clf__alpha = 1e-04, \\\n",
    "                               clf__n_iter = 50, \\\n",
    "                               clf__penalty = 'l2', \\\n",
    "                               clf__n_jobs = -1, \\\n",
    "                               tfidf__norm = 'l2', \\\n",
    "                               tfidf__use_idf = True, \\\n",
    "                               vect__max_df = 0.4, \\\n",
    "                               vect__ngram_range = (1, 2), \\\n",
    "#                                pca__n_components = 100\n",
    "                              )\n",
    "\n",
    "\"\"\"\n",
    "text_clf = text_clf.set_params(\\\n",
    "                               clf__alpha = 0.0001, \\\n",
    "                               clf__n_iter = 100, \\\n",
    "                               clf__penalty = 'l2', \\\n",
    "                               tfidf__norm = 'l2', \\\n",
    "                               tfidf__use_idf = False, \\\n",
    "                               vect__max_df = 0.75, \\\n",
    "                               vect__max_features = 10000, \\\n",
    "                               vect__ngram_range = (1, 2)\n",
    "                              )\n",
    "        \n",
    "\n",
    "text_clf = text_clf.set_params(clf__criterion = 'gini', \\\n",
    "                               clf__max_features = 'auto', \\\n",
    "                               clf__n_estimators = 50, \\\n",
    "                               clf__n_jobs = -1, \\\n",
    "                               tfidf__norm = 'l2', \\\n",
    "                               tfidf__use_idf = False, \\\n",
    "                               vect__max_df = 1, \\\n",
    "                               vect__max_features = None, \\\n",
    "                               vect__ngram_range = (1, 2), \\\n",
    "#                                pca__n_components = 100\n",
    "                              )\n",
    "\"\"\"\n",
    "t0 = time()\n",
    "\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "print('Training the classifier took %0.3fs' % (t1 - t0))\n",
    "\n",
    "y_train_pred = text_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "#print(np.mean(y_train == y_train_pred))\n",
    "\n",
    "# print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827733659133\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.95      0.73      0.82       215\n",
      "          2       0.33      1.00      0.50         1\n",
      "          4       0.00      0.00      0.00         1\n",
      "         10       0.00      0.00      0.00         1\n",
      "        100       1.00      0.08      0.15        12\n",
      "        120       0.00      0.00      0.00         1\n",
      "        500       0.00      0.00      0.00         2\n",
      "       1000       0.75      0.98      0.85       537\n",
      "       1500       0.50      1.00      0.67         1\n",
      "       2000       1.00      0.50      0.67         6\n",
      "       3264       0.00      0.00      0.00         1\n",
      "       3306       0.00      0.00      0.00         1\n",
      "       3606       0.00      0.00      0.00         1\n",
      "       4247       0.00      0.00      0.00         1\n",
      "       4800       0.00      0.00      0.00         1\n",
      "       5000       0.80      0.27      0.40        15\n",
      "       6146       0.00      0.00      0.00         1\n",
      "       8688       0.00      0.00      0.00         1\n",
      "      10000       0.91      0.75      0.82        80\n",
      "      16810       0.00      0.00      0.00         1\n",
      "      20000       0.00      0.00      0.00         2\n",
      "      20450       0.00      0.00      0.00         1\n",
      "      30000       0.00      0.00      0.00         1\n",
      "      46210       0.00      0.00      0.00         1\n",
      "      50000       0.92      0.97      0.95       120\n",
      "      75000       0.00      0.00      0.00         1\n",
      "     100000       0.81      0.71      0.75        65\n",
      "     112700       0.00      0.00      0.00         1\n",
      "     143000       0.00      0.00      0.00         1\n",
      "     150000       0.00      0.00      0.00         2\n",
      "     200000       0.81      0.59      0.68        22\n",
      "     225000       0.00      0.00      0.00         1\n",
      "     250000       0.86      0.50      0.63        12\n",
      "     300000       0.67      0.67      0.67         6\n",
      "     339000       0.00      0.00      0.00         1\n",
      "     339300       0.00      0.00      0.00         0\n",
      "     350000       1.00      0.50      0.67         2\n",
      "     393000       0.00      0.00      0.00         1\n",
      "     400000       1.00      0.29      0.44         7\n",
      "     401000       0.00      0.00      0.00         1\n",
      "     440000       0.00      0.00      0.00         0\n",
      "     500000       0.69      0.85      0.76        72\n",
      "     507500       0.00      0.00      0.00         1\n",
      "     532900       0.00      0.00      0.00         1\n",
      "     590000       0.00      0.00      0.00         1\n",
      "     700000       0.00      0.00      0.00         1\n",
      "    1000000       0.86      0.77      0.81        88\n",
      "    1004000       0.00      0.00      0.00         1\n",
      "    1005000       0.00      0.00      0.00         1\n",
      "    1014000       0.00      0.00      0.00         1\n",
      "    1106000       0.00      0.00      0.00         1\n",
      "    1421000       0.00      0.00      0.00         1\n",
      "    1500000       0.00      0.00      0.00         1\n",
      "    2000000       0.33      0.33      0.33         3\n",
      "    2006000       0.00      0.00      0.00         1\n",
      "    2045000       0.00      0.00      0.00         1\n",
      "    2325000       0.00      0.00      0.00         1\n",
      "    2900000       0.00      0.00      0.00         1\n",
      "    2990000       0.00      0.00      0.00         1\n",
      "    3000000       0.50      0.50      0.50         2\n",
      "    3033000       0.00      0.00      0.00         1\n",
      "    3200000       0.00      0.00      0.00         1\n",
      "    3900000       0.00      0.00      0.00         1\n",
      "    4000000       1.00      1.00      1.00         1\n",
      "    5000000       0.86      0.86      0.86        14\n",
      "    5800000       0.00      0.00      0.00         1\n",
      "    6158000       0.00      0.00      0.00         1\n",
      "    6500000       0.00      0.00      0.00         0\n",
      "    8000000       0.00      0.00      0.00         1\n",
      "    8785000       0.00      0.00      0.00         1\n",
      "    9139000       0.00      0.00      0.00         1\n",
      "   10000000       0.91      0.97      0.94        99\n",
      "   10700000       0.00      0.00      0.00         1\n",
      "   11800000       0.00      0.00      0.00         1\n",
      "   15870000       0.00      0.00      0.00         1\n",
      "   20000000       0.00      0.00      0.00         1\n",
      "   23030000       0.00      0.00      0.00         1\n",
      "   25000000       0.97      0.99      0.98        97\n",
      "   30000000       1.00      0.88      0.93         8\n",
      "   35000000       1.00      0.67      0.80         3\n",
      "   40000000       1.00      1.00      1.00         1\n",
      "   50000000       0.96      0.92      0.94        25\n",
      "   50080000       0.00      0.00      0.00         1\n",
      "   51300000       0.00      0.00      0.00         1\n",
      "   80000000       1.00      1.00      1.00         1\n",
      "  100000000       0.91      0.80      0.85        54\n",
      "  105000000       0.00      0.00      0.00         1\n",
      "  110600000       0.00      0.00      0.00         1\n",
      "  112400000       0.00      0.00      0.00         1\n",
      "  150000000       0.00      0.00      0.00         1\n",
      "  300000000       1.00      1.00      1.00         1\n",
      "  500000000       0.00      0.00      0.00         1\n",
      "  500200000       0.00      0.00      0.00         1\n",
      "  500300000       0.00      0.00      0.00         1\n",
      " 1000000000       0.00      0.00      0.00         1\n",
      " 2000000000       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.81      0.83      0.81      1637\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ciprian/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ciprian/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(np.mean(y_test_pred == y_test))\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0900045c833fd034', 'XS0461332347', 'USD', 2000.0, 1000.0, 'N',\n",
       "        'USUB', 'N', 'USD', 'DEUTSCHE BANK AG, GREAT WINCHE', 'LONDON',\n",
       "        'GERMANY', nan, nan, nan]], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv( datafolder + \"labels.csv\", header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "get_labels_for_isin(labels,'XS0461332347')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.0001, 1e-05, 1e-06),\n",
      " 'clf__n_iter': (10, 50, 100),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__max_features': (None, 5000, 10000, 50000),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 1728 candidates, totalling 5184 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ciprian/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 688 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1092 tasks      | elapsed:   33.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1892 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3092 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3842 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4692 tasks      | elapsed:  3.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 258.210s\n",
      "\n",
      "Best score: 0.810\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.0001\n",
      "\tclf__n_iter: 10\n",
      "\tclf__penalty: 'l2'\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__max_df: 0.75\n",
      "\tvect__max_features: 10000\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 5184 out of 5184 | elapsed:  4.3min finished\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dataset, labelset = load_and_sort_training_data('mult_labeled_docs_by_isin.pickle')\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(dataset, labelset,\\\n",
    "                                                                     test_size=0.3, random_state=53)\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer()), \\\n",
    "                     ('tfidf', TfidfTransformer()), \\\n",
    "                     ('clf', SGDClassifier()) \\\n",
    "#                     ('clf', RandomForestClassifier())\n",
    "                    ])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\\\n",
    "#               'clf__n_estimators' : (10, 50, 100), \\\n",
    "#               'clf__criterion' : ('entropy', 'gini'), \\\n",
    "#               'clf__max_features' : ('auto', 'sqrt', 'log2'), \\\n",
    "              'vect__max_df': (0.5, 0.75, 1.0), \\\n",
    "              'vect__max_features': (None, 5000, 10000, 50000), \\\n",
    "              'vect__ngram_range': ((1, 1), (1, 2)), \\\n",
    "              'tfidf__use_idf': (True, False), \\\n",
    "              'tfidf__norm': ('l1', 'l2'), \\\n",
    "              'clf__alpha': (0.0001, 0.00001, 0.000001),\\\n",
    "              'clf__penalty': ('l2', 'elasticnet'),\\\n",
    "              'clf__n_iter': (10, 50, 100)\\\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train[:1000], y_train[:1000])\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multiple_amount_pipeline.pkl',\n",
       " 'multiple_amount_pipeline.pkl_01.npy',\n",
       " 'multiple_amount_pipeline.pkl_02.npy',\n",
       " 'multiple_amount_pipeline.pkl_03.npy',\n",
       " 'multiple_amount_pipeline.pkl_04.npy',\n",
       " 'multiple_amount_pipeline.pkl_05.npy',\n",
       " 'multiple_amount_pipeline.pkl_06.npy']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "dataset, labelset = load_and_sort_training_data('mult_labeled_docs_by_isin.pickle')\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \\\n",
    "                     ('tfidf', TfidfTransformer()), \\\n",
    "                     ('clf', SGDClassifier())])\n",
    "\n",
    "text_clf = text_clf.set_params(clf__alpha = 1e-04, \\\n",
    "                               clf__n_iter = 50, \\\n",
    "                               clf__penalty = 'l2', \\\n",
    "                               clf__n_jobs = -1, \\\n",
    "                               tfidf__norm = 'l2', \\\n",
    "                               tfidf__use_idf = True, \\\n",
    "                               vect__max_df = 0.4, \\\n",
    "                               vect__ngram_range = (1, 2))\n",
    "\n",
    "text_clf = text_clf.fit(dataset, labelset)\n",
    "\n",
    "joblib.dump(text_clf, 'multiple_amount_pipeline.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCID</th>\n",
       "      <th>ISIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0900045c8059e795</td>\n",
       "      <td>CH0025370906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0900045c8092c413</td>\n",
       "      <td>XS0545673914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0900045c80c5dd9f</td>\n",
       "      <td>US40430CLG23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0900045c80d7461e</td>\n",
       "      <td>US44328MAX20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0900045c80dd8470</td>\n",
       "      <td>XS0729081124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DOCID          ISIN\n",
       "0  0900045c8059e795  CH0025370906\n",
       "1  0900045c8092c413  XS0545673914\n",
       "2  0900045c80c5dd9f  US40430CLG23\n",
       "3  0900045c80d7461e  US44328MAX20\n",
       "4  0900045c80dd8470  XS0729081124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_isins = pd.read_csv('../final_test/docID/docid_final_test.csv', header=0, sep=\",\", quoting=1, thousands=\",\")\n",
    "\n",
    "# unlabeled_isins = load_isins_files('../int_test/docID/docid_int_test.csv')\n",
    "\n",
    "unlabeled_isins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>OPS.CURR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XS1330099810</td>\n",
       "      <td>RUB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XS1326148142</td>\n",
       "      <td>SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XS1129852551</td>\n",
       "      <td>JPY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XS1276919328</td>\n",
       "      <td>EUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XS1317265038</td>\n",
       "      <td>USD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ISIN OPS.CURR\n",
       "0  XS1330099810      RUB\n",
       "1  XS1326148142      SGD\n",
       "2  XS1129852551      JPY\n",
       "3  XS1276919328      EUR\n",
       "4  XS1317265038      USD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_currencies = pd.read_csv('currencyExtraction/optCur.csv', header=None, names=['ISIN', 'OPS.CURR'], sep=\",\", quoting=1, thousands=\",\")\n",
    "labeled_currencies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Finished grouping file contents indexed by ISIN\n",
      "Saved file contents indexed by ISIN to: unlabeled_amt_docs_by_isin.pickle\n",
      "Processed 424 files in 25.780s\n",
      "OK!\n"
     ]
    }
   ],
   "source": [
    "group_docs_by_isin(html1_folder, labeled_currencies, unlabeled_isins, pickle_results=True, pickle_file='unlabeled_amt_docs_by_isin.pickle')\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "unlabeled_data = pickle.load(open('unlabeled_amt_docs_by_isin.pickle', 'rb'))\n",
    "data_by_isin = [[k, v] for k, v in unlabeled_data.items()]\n",
    "data_by_isin_df = pd.DataFrame(data_by_isin, columns = ['isin','content'])\n",
    "data_by_isin_df.sort_values('isin', axis=0, inplace=True)\n",
    "  \n",
    "unlabeled_dataset = data_by_isin_df.as_matrix(columns=['content']).flatten()\n",
    "isins = data_by_isin_df.as_matrix(columns=['isin']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#load the classifier\n",
    "clf = joblib.load('multiple_amount_pipeline.pkl')\n",
    "multiple_amounts = clf.predict(unlabeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "results_df = pd.DataFrame(np.vstack((isins, multiple_amounts)).T, columns=['ISIN', 'MLT.TRAD.AMT'])\n",
    "\n",
    "results_df.to_csv('final_mltTrad.csv', index=False, header=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
